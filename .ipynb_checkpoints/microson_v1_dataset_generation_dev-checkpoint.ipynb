{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d9331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import masp as srs\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import tqdm\n",
    "import pyrubberband as pyrb\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4806dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_path = '/home/ubuntu/Data/mls_spanish'\n",
    "wham_path = '/home/ubuntu/Data/wham'\n",
    "output_path = '/home/ubuntu/Data/microson_v1/'\n",
    "decoder_path = 'ku100_inear_test.mat'\n",
    "fs = 16000\n",
    "df = pd.read_csv('meta_microson_v1.csv')\n",
    "ambi_order = 10\n",
    "rims_d = .0\n",
    "maxlim = 2.\n",
    "band_centerfreqs=np.array([1000])\n",
    "\n",
    "decoder = mat73.loadmat(decoder_path)['hnm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4bd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dirs\n",
    "sets = ['train', 'dev', 'test']\n",
    "for subset in sets:\n",
    "    if not os.path.exists(pjoin(output_path, subset)):\n",
    "        os.makedirs(pjoin(output_path, subset))\n",
    "    if not os.path.exists(pjoin(pjoin(output_path, subset), 'anechoic')):\n",
    "        os.makedirs(pjoin(pjoin(output_path, subset), 'anechoic'))\n",
    "    if not os.path.exists(pjoin(pjoin(output_path, subset), 'reverberant')):\n",
    "        os.makedirs(pjoin(pjoin(output_path, subset), 'reverberant'))\n",
    "    if not os.path.exists(pjoin(pjoin(output_path, subset), 'noise')):\n",
    "        os.makedirs(pjoin(pjoin(output_path, subset), 'noise'))\n",
    "    if not os.path.exists(pjoin(pjoin(output_path, subset), 'ir')):\n",
    "        os.makedirs(pjoin(pjoin(output_path, subset), 'ir'))\n",
    "    if not os.path.exists(pjoin(pjoin(output_path, subset), 'ane_ir')):\n",
    "        os.makedirs(pjoin(pjoin(output_path, subset), 'ane_ir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df365448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we copy the metadata file to the output dir\n",
    "df.to_csv(pjoin(output_path, 'meta_microson_v1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80596d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.iloc[203964]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1f09ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "(268800,)\n",
      "116736\n",
      "(64000,)\n",
      "(2, 64000)\n"
     ]
    }
   ],
   "source": [
    "mic = np.array(hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "                                       np.array([a.headOrient_azi,a.headOrient_ele])))\n",
    "# load noise:\n",
    "noise, _ = sf.read(pjoin(pjoin(pjoin(wham_path, 'wham_noise'), a.wham_split), a.noise_path))\n",
    "\n",
    "# time stretch if needed\n",
    "if a.stretch != 0.0:\n",
    "    noise = pyrb.time_stretch(noise, a.fs_noise, a.stretch)\n",
    "\n",
    "# extend if needed with hanning window\n",
    "noise = np.array([hlp.extend_noise(noise[:,0], a.num_chunks * 4 * a.fs_noise, a.fs_noise),\n",
    "         hlp.extend_noise(noise[:,1], a.num_chunks * 4 * a.fs_noise, a.fs_noise)]).T\n",
    "\n",
    "# crop 4 seconds chunk\n",
    "noise = noise[a.chunk * 4 * a.fs_noise:(a.chunk + 1) * 4 * a.fs_noise]\n",
    "\n",
    "# invert phase for augmentation\n",
    "if a.phase_inv:\n",
    "    noise *= -1\n",
    "\n",
    "# invert channels for augmentation\n",
    "if a.lr_inv:\n",
    "    noise = noise[:, [1,0]]\n",
    "\n",
    "noise = noise.T\n",
    "print(a.fs_noise)\n",
    "# load speech and crop at the 4s chunk that has more energy\n",
    "speech_folder = pjoin(pjoin(mls_path, a.mls_split), 'audio')\n",
    "speech, _ = sf.read(pjoin(pjoin(pjoin(speech_folder, str(a.speaker)), str(a.book)), a.speech_path))\n",
    "print(speech.shape)\n",
    "env = sig.fftconvolve(speech, np.ones(4*a.fs_noise), 'same')\n",
    "idx_candidates = np.flip(np.argsort(env**2))\n",
    "idx = idx_candidates[idx_candidates < (len(speech)-(4*a.fs_noise))][0]\n",
    "print(idx)\n",
    "speech = speech[idx:idx+4*a.fs_noise]\n",
    "print(speech.shape)\n",
    "print(noise.shape)\n",
    "room = np.array([a.room_x, a.room_y, a.room_z])\n",
    "rt60 = np.array([a.rt60])\n",
    "rt60 *= 0.5#furniture absorption? \n",
    "#snr 0, more people, more reduction -> 0.3 * rt60\n",
    "#snr 5, less people, no rt60 reduction -> 1.0 * rt60\n",
    "rt60 *= ((a.snr+0.3)/5.3) # people absoprtion\n",
    "src = np.array([[a.src_x, a.src_y, a.src_z]])\n",
    "nRec = mic.shape[0]\n",
    "\n",
    "nSrc = src.shape[0]\n",
    "\n",
    "head_orient = np.array([a.headOrient_azi, a.headOrient_ele])\n",
    "\n",
    "# Compute absorption coefficients for desired rt60 and room dimensions\n",
    "abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "# Small correction for sound absorption coefficients:\n",
    "if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "\n",
    "# Generally, we simulate up to RT60:\n",
    "limits = np.minimum(rt60, maxlim)\n",
    "\n",
    "abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, rims_d, head_orient)\n",
    "ane_echograms = hlp.crop_echogram(copy.deepcopy(abs_echograms))\n",
    "mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs)\n",
    "ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs)\n",
    "#hlp.plot_scene(room,np.array([a.headC_x, a.headC_y, a.headC_z]),head_orient,mic,src,perspective=\"xy\")\n",
    "bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                    sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                    sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "reverberant_src = np.array([sig.fftconvolve(speech, bin_ir[0, :], 'same'), sig.fftconvolve(speech, bin_ir[1, :], 'same')])\n",
    "anechoic_src = np.array([sig.fftconvolve(speech, bin_aneIR[0, :], 'same'), sig.fftconvolve(speech, bin_aneIR[1, :], 'same')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451db516",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_snr = 10 * np.log10(hlp.power(reverberant_src) / hlp.power(noise) + np.finfo(noise.dtype).resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6f89e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "(268800,)\n",
      "116736\n",
      "(64000,)\n",
      "(2, 64000)\n"
     ]
    }
   ],
   "source": [
    "noise_gain_db = ini_snr - a.snr\n",
    "\n",
    "noise = noise * np.power(10, noise_gain_db/20)\n",
    "norm_fact = np.max(np.abs(reverberant_src + noise))\n",
    "\n",
    "anechoic_src /= norm_fact\n",
    "noise /= norm_fact\n",
    "reverberant_src /= norm_fact\n",
    "\n",
    "anechoic_src *= 0.99\n",
    "noise *= 0.99\n",
    "reverberant_src *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab795ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df.iloc[i]\n",
    "def process(a):\n",
    "    mic = np.array(hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "                                           np.array([a.headOrient_azi,a.headOrient_ele])))\n",
    "    # load noise:\n",
    "    noise, _ = sf.read(pjoin(pjoin(pjoin(wham_path, 'wham_noise'), a.wham_split), a.noise_path))\n",
    "\n",
    "    # time stretch if needed\n",
    "    if a.stretch != 0.0:\n",
    "        noise = pyrb.time_stretch(noise, a.fs_noise, a.stretch)\n",
    "\n",
    "    # extend if needed with hanning window\n",
    "    noise = np.array([hlp.extend_noise(noise[:,0], a.num_chunks * 4 * a.fs_noise, a.fs_noise),\n",
    "             hlp.extend_noise(noise[:,1], a.num_chunks * 4 * a.fs_noise, a.fs_noise)]).T\n",
    "\n",
    "    # crop 4 seconds chunk\n",
    "    noise = noise[a.chunk * 4 * a.fs_noise:(a.chunk + 1) * 4 * a.fs_noise]\n",
    "\n",
    "    # invert phase for augmentation\n",
    "    if a.phase_inv:\n",
    "        noise *= -1\n",
    "\n",
    "    # invert channels for augmentation\n",
    "    if a.lr_inv:\n",
    "        noise = noise[:, [1,0]]\n",
    "\n",
    "    noise = noise.T\n",
    "    print(a.fs_noise)\n",
    "    # load speech and crop at the 4s chunk that has more energy\n",
    "    speech_folder = pjoin(pjoin(mls_path, a.mls_split), 'audio')\n",
    "    speech, _ = sf.read(pjoin(pjoin(pjoin(speech_folder, str(a.speaker)), str(a.book)), a.speech_path))\n",
    "    print(speech.shape)\n",
    "    env = sig.fftconvolve(speech, np.ones(4*a.fs_noise), 'same')\n",
    "    idx_candidates = np.flip(np.argsort(env**2))\n",
    "    idx = idx_candidates[idx_candidates < (len(speech)-(4*a.fs_noise))][0]\n",
    "    print(idx)\n",
    "    speech = speech[idx:idx+4*a.fs_noise]\n",
    "    print(speech.shape)\n",
    "    print(noise.shape)\n",
    "    room = np.array([a.room_x, a.room_y, a.room_z])\n",
    "    rt60 = np.array([a.rt60])\n",
    "    rt60 *= 0.5#furniture absorption? \n",
    "    #snr 0, more people, more reduction -> 0.3 * rt60\n",
    "    #snr 5, less people, no rt60 reduction -> 1.0 * rt60\n",
    "    rt60 *= ((a.snr+0.3)/5.3) # people absoprtion\n",
    "    src = np.array([[a.src_x, a.src_y, a.src_z]])\n",
    "    nRec = mic.shape[0]\n",
    "\n",
    "    nSrc = src.shape[0]\n",
    "\n",
    "    head_orient = np.array([a.headOrient_azi, a.headOrient_ele])\n",
    "\n",
    "    # Compute absorption coefficients for desired rt60 and room dimensions\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "    # Small correction for sound absorption coefficients:\n",
    "    if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "        abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "\n",
    "    # Generally, we simulate up to RT60:\n",
    "    limits = np.minimum(rt60, maxlim)\n",
    "\n",
    "    abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, rims_d, head_orient)\n",
    "    ane_echograms = hlp.crop_echogram(copy.deepcopy(abs_echograms))\n",
    "    mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs)\n",
    "    ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs)\n",
    "    #hlp.plot_scene(room,np.array([a.headC_x, a.headC_y, a.headC_z]),head_orient,mic,src,perspective=\"xy\")\n",
    "    bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    reverberant_src = np.array([sig.fftconvolve(speech, bin_ir[0, :], 'same'), sig.fftconvolve(speech, bin_ir[1, :], 'same')])\n",
    "    anechoic_src = np.array([sig.fftconvolve(speech, bin_aneIR[0, :], 'same'), sig.fftconvolve(speech, bin_aneIR[1, :], 'same')])\n",
    "    ini_snr = 10 * np.log10(hlp.power(reverberant_src) / hlp.power(noise) + np.finfo(noise.dtype).resolution)\n",
    "\n",
    "\n",
    "\n",
    "    noise_gain_db = ini_snr - a.snr\n",
    "\n",
    "    noise = noise * np.power(10, noise_gain_db/20)\n",
    "    norm_fact = np.max(np.abs(reverberant_src + noise))\n",
    "\n",
    "    anechoic_src /= norm_fact\n",
    "    noise /= norm_fact\n",
    "    reverberant_src /= norm_fact\n",
    "\n",
    "    anechoic_src *= 0.99\n",
    "    noise *= 0.99\n",
    "    reverberant_src *= 0.99\n",
    "    \n",
    "    writepath = pjoin(output_path, a.mls_split)\n",
    "    sf.write(pjoin(pjoin(writepath, 'anechoic'), os.path.splitext(a.speech_path)[0]+'.wav'), anechoic_src.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'reverberant'), os.path.splitext(a.speech_path)[0]+'.wav'), reverberant_src.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'noise'), os.path.splitext(a.speech_path)[0]+'.wav'), noise.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_ir.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ane_ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_aneIR.T, fs, subtype='FLOAT')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd871833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process(df.iloc[42318])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea726551",
   "metadata": {},
   "outputs": [],
   "source": [
    "16000*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1b918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibriMix3D",
   "language": "python",
   "name": "librimix3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
