{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d9331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import masp as srs\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import tqdm\n",
    "import pyrubberband as pyrb\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df.iloc[i]\n",
    "def process(a):\n",
    "    mic = np.array(hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "                                           np.array([a.headOrient_azi,a.headOrient_ele])))\n",
    "    # load noise:\n",
    "    noise, _ = sf.read(pjoin(pjoin(pjoin(wham_path, 'wham_noise'), a.wham_split), a.noise_path))\n",
    "\n",
    "    # time stretch if needed\n",
    "    if a.stretch != 0.0:\n",
    "        noise = pyrb.time_stretch(noise, a.fs_noise, a.stretch)\n",
    "\n",
    "    # extend if needed with hanning window\n",
    "    noise = np.array([hlp.extend_noise(noise[:,0], a.num_chunks * 4 * a.fs_noise, a.fs_noise),\n",
    "             hlp.extend_noise(noise[:,1], a.num_chunks * 4 * a.fs_noise, a.fs_noise)]).T\n",
    "\n",
    "    # crop 4 seconds chunk\n",
    "    noise = noise[a.chunk * 4 * a.fs_noise:(a.chunk + 1) * 4 * a.fs_noise]\n",
    "\n",
    "    # invert phase for augmentation\n",
    "    if a.phase_inv:\n",
    "        noise *= -1\n",
    "\n",
    "    # invert channels for augmentation\n",
    "    if a.lr_inv:\n",
    "        noise = noise[:, [1,0]]\n",
    "\n",
    "    noise = noise.T\n",
    "    print(a.fs_noise)\n",
    "    # load speech and crop at the 4s chunk that has more energy\n",
    "    speech_folder = pjoin(pjoin(mls_path, a.mls_split), 'audio')\n",
    "    speech, _ = sf.read(pjoin(pjoin(pjoin(speech_folder, str(a.speaker)), str(a.book)), a.speech_path))\n",
    "    print(speech.shape)\n",
    "    env = sig.fftconvolve(speech, np.ones(4*a.fs_noise), 'same')\n",
    "    idx_candidates = np.flip(np.argsort(env**2))\n",
    "    idx = idx_candidates[idx_candidates < (len(speech)-(4*a.fs_noise))][0]\n",
    "    print(idx)\n",
    "    speech = speech[idx:idx+4*a.fs_noise]\n",
    "    print(speech.shape)\n",
    "    print(noise.shape)\n",
    "    room = np.array([a.room_x, a.room_y, a.room_z])\n",
    "    rt60 = np.array([a.rt60])\n",
    "    rt60 *= 0.5#furniture absorption? \n",
    "    #snr 0, more people, more reduction -> 0.3 * rt60\n",
    "    #snr 5, less people, no rt60 reduction -> 1.0 * rt60\n",
    "    rt60 *= ((a.snr+0.3)/5.3) # people absoprtion\n",
    "    src = np.array([[a.src_x, a.src_y, a.src_z]])\n",
    "    nRec = mic.shape[0]\n",
    "\n",
    "    nSrc = src.shape[0]\n",
    "\n",
    "    head_orient = np.array([a.headOrient_azi, a.headOrient_ele])\n",
    "\n",
    "    # Compute absorption coefficients for desired rt60 and room dimensions\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "    # Small correction for sound absorption coefficients:\n",
    "    if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "        abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "\n",
    "    # Generally, we simulate up to RT60:\n",
    "    limits = np.minimum(rt60, maxlim)\n",
    "    mic_specs = np.array([[1, 0, 0, 1]])\n",
    "    abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, rims_d, head_orient)\n",
    "    mono_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, mic_specs, rims_d)\n",
    "    ane_echograms = hlp.crop_echogram(copy.deepcopy(abs_echograms))\n",
    "    mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs)\n",
    "    mono_rirs = srs.render_rirs_mic(mono_echograms, band_centerfreqs, fs)\n",
    "    ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs)\n",
    "    #hlp.plot_scene(room,np.array([a.headC_x, a.headC_y, a.headC_z]),head_orient,mic,src,perspective=\"xy\")\n",
    "    bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    reverberant_src = np.array([sig.fftconvolve(speech, bin_ir[0, :], 'same'), sig.fftconvolve(speech, bin_ir[1, :], 'same')])\n",
    "    anechoic_src = np.array([sig.fftconvolve(speech, bin_aneIR[0, :], 'same'), sig.fftconvolve(speech, bin_aneIR[1, :], 'same')])\n",
    "    ini_snr = 10 * np.log10(hlp.power(reverberant_src) / hlp.power(noise) + np.finfo(noise.dtype).resolution)\n",
    "\n",
    "\n",
    "\n",
    "    noise_gain_db = ini_snr - a.snr\n",
    "\n",
    "    noise = noise * np.power(10, noise_gain_db/20)\n",
    "    norm_fact = np.max(np.abs(reverberant_src + noise))\n",
    "\n",
    "    anechoic_src /= norm_fact\n",
    "    noise /= norm_fact\n",
    "    reverberant_src /= norm_fact\n",
    "\n",
    "    anechoic_src *= 0.99\n",
    "    noise *= 0.99\n",
    "    reverberant_src *= 0.99\n",
    "    \n",
    "    writepath = pjoin(output_path, a.mls_split)\n",
    "    sf.write(pjoin(pjoin(writepath, 'anechoic'), os.path.splitext(a.speech_path)[0]+'.wav'), anechoic_src.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'reverberant'), os.path.splitext(a.speech_path)[0]+'.wav'), reverberant_src.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'noise'), os.path.splitext(a.speech_path)[0]+'.wav'), noise.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_ir.T, fs, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ane_ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_aneIR.T, fs, subtype='FLOAT')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd871833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process(df.iloc[42318])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea726551",
   "metadata": {},
   "outputs": [],
   "source": [
    "16000*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1b918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibriMix3D",
   "language": "python",
   "name": "librimix3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
